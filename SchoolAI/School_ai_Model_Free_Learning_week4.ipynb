{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "School_ai_Model_Free_Learning_week4",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "NPRcOi77dLkU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Associate Learning Theory**\n",
        ">![Accociative Learning](Img/Associate_Learning_Theory.png)\n",
        "\n",
        "**Rescorla-Wagner Model** <br>\n",
        "$\\Delta V_n=c(\\lambda-V_{n-1})$ <br>\n",
        "$\\Delta V_n$: change in associative strength for CS on one trial <br>\n",
        "$c$: represents salience of CS and US; a constant (0.0-1.0) <br>\n",
        "$\\lambda$:maximum associative strength (magnitude of UR) <br>\n",
        "$V_{n-1}$: associative strength already accured by CS <br>\n",
        "CS = conditioned stimuli <br>\n",
        "US = unconditioned stimuli <br>\n",
        "\n",
        "**Groundbreaking**:\n",
        "* Able to explain fear conditioning phenomena\n",
        "* Useful in early natural language processing systems\n",
        "\n",
        "**Bayes Rule** $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
        "<br>\n",
        "\n",
        "|  | Model Free | Model Based |\n",
        "| :---         |     :---:      |          ---: |\n",
        "|    | less online deliberation     | More online deliberation    |\n",
        "| Learn     | policy $\\pi$      | Model of R and T      |\n",
        "| Online     | $\\pi(s)$       | Solve MDP      |\n",
        "| Method     | Learning from demonstration       | Adaptive dynamic programming      |\n",
        "|      | Simpler execution       | Fewer examples needed to learn?      |\n",
        "\n",
        "<br>\n",
        "\n",
        ">![Model Base](Img/Model_Based_RL.png)\n",
        "\n",
        "Model free is fast but it is inflexible, if the reward functions all the lookup table has to change whereas in the model based learning system it is nore flexible. If the reward at some state changes we just change the reward function at that state and that reward will propagate to all of our values via the reward function we are defined but it is slower.\n",
        "\n",
        "+ _Associative Learning_ is a learning process in which a new response becomes associated with a particular stimulus. <br>\n",
        "+ When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainty about the world. <br>\n",
        "+ Temporal difference learning is a model free learning technique that predicts the expected value of a variable occurring at the end of a sequence of states. <br>\n"
      ]
    },
    {
      "metadata": {
        "id": "_dJs_Mt2dEFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}