{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic_Programming.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RFsVTMU3tnng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Theory of Dynamic Programming\n",
        "\n",
        "Dynamic Programming is a method for solving complex problem by breaking down into subproblems and combine solutions of subproblems. Dynamic Programming is a very general solution method for problems which have two properties:\n",
        "1.  Optimal substructure \n",
        "  *  Principle of optimality applies \n",
        "  *  Optimal solution can be decomposed into subproblems\n",
        "2.  Overlapping subprobems\n",
        "  * Subproblems recur many times\n",
        "  * Solutions can be cached and reused\n",
        "  \n",
        "Markov decicion processes satisfy both properties\n",
        "  * Bellman equation gives recursive decomposition\n",
        "  * Value function stores and reuses solutions"
      ]
    },
    {
      "metadata": {
        "id": "Zxrs8irWEC0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "e6c19509-6391-4e41-e28d-bcf58a0e9773"
      },
      "cell_type": "code",
      "source": [
        "! pip install gym"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/44/3a63e8b87f642db49ac81239620e68df8cfae223dcfda4f8508aec88d204/gym-0.10.8.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.8.24)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/ec/dd/33bcc8801d345f0b640fced8a0864a7c8474828564bc5ccf70\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.8 pyglet-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "atmej6oyAmpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83e3727c-f55c-4a0d-ae81-d1b2ebf846ed"
      },
      "cell_type": "code",
      "source": [
        "# Use Brute Force to find best solution \n",
        "import numpy\n",
        "import time\n",
        "import gym\n",
        "\n",
        "\"\"\"\n",
        "Args:\n",
        "policy: [S, A] shaped matrix representing the policy.\n",
        "env: OpenAI gym env.\n",
        "render: boolean to turn rendering on/off.\n",
        "\"\"\"\n",
        "\n",
        "#Execution\n",
        "def execute(env, policy, episodeLength=100, render=False):\n",
        "  totalReward = 0\n",
        "  start = env.reset()\n",
        "  for t in range(episodeLength):\n",
        "    if render:\n",
        "      env.render()\n",
        "    action = policy[start]\n",
        "    start, reward, done, _ = env.step(action)\n",
        "    totalReward += reward\n",
        "    if done:\n",
        "      break\n",
        "  return totalReward\n",
        "\n",
        "#Evaluation\n",
        "def evaluatePolicy(env, policy, n_episodes=100):\n",
        "  totalReward = 0.0\n",
        "  for _ in range(n_episodes):\n",
        "    totalReward += execute(env, policy)\n",
        "  return totalReward / n_episodes\n",
        "\n",
        "#Function for a random policy\n",
        "def gen_random_policy():\n",
        "  return numpy.random.choice(4, size=((16)))\n",
        " \n",
        "if __name__ == '__main__':\n",
        "  env = gym.make('FrozenLake-v0')\n",
        "  \n",
        "## Policy search\n",
        "n_policies = 1000\n",
        "startTime = time.time()\n",
        "policy_set = [gen_random_policy() for _ in range(n_policies)]\n",
        "policy_score = [evaluatePolicy(env, p) for p in policy_set]\n",
        "endTime = time.time()\n",
        "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(numpy.max(policy_score), endTime - startTime)) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score = 0.35. Time taken = 8.4057 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IxGgC-nEJWup",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Value Iteration](..Img/Value_iteration_qseudocode.PNG)\n"
      ]
    },
    {
      "metadata": {
        "id": "ZtrEfmNGHG-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ce595e9-2c53-4520-a3a2-f79c0066bccf"
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import gym\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Args:\n",
        "policy: [S, A] shaped matrix representing the policy.\n",
        "env: OpenAI gym env.\n",
        "env.P represents the transition probabilities of the environment.\n",
        "env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
        "env.nS is a number of states in the environment.\n",
        "env.nA is a number of actions in the environment.\n",
        "gamma: Gamma discount factor.\n",
        "render: boolean to turn rendering on/off.\n",
        "\"\"\"\n",
        "\n",
        "def execute(env, policy, gamma=1.0, render=False):\n",
        "  start = env.reset()\n",
        "  totalReward = 0\n",
        "  stepIndex = 0\n",
        "  while True:\n",
        "    if render:\n",
        "      env.render() \n",
        "    start, reward, done, _ = env.step(int(policy[start]))\n",
        "    totalReward += (gamma ** stepIndex * reward)\n",
        "    stepIndex += 1\n",
        "    if done:\n",
        "      break\n",
        "  return totalReward\n",
        "\n",
        "# Evaluates a policy by running it n times.returns:average total reward\n",
        "def evaluatePolicy(env, policy, gamma=1.0, n=100):\n",
        "  scores = [\n",
        "    execute(env, policy, gamma=gamma, render=False)\n",
        "    for _ in range(n)]\n",
        "  return numpy.mean(scores)\n",
        "\n",
        "# choosing the policy given a value-function\n",
        "def calculatePolicy(v, gamma=1.0):\n",
        "  policy = numpy.zeros(env.env.nS)\n",
        "  for s in range(env.env.nS):\n",
        "    q_sa = numpy.zeros(env.action_space.n)\n",
        "    for a in range(env.action_space.n):\n",
        "      for next_sr in env.env.P[s][a]:\n",
        "        # next_sr is a tuple of (probability, next state, reward, done)\n",
        "        p, s_, r, _ = next_sr\n",
        "        q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "    policy[s] = numpy.argmax(q_sa)\n",
        "  return policy\n",
        "\n",
        "# Value Iteration Agorithm\n",
        "def valueIteration(env, gamma=1.0):\n",
        "  value = numpy.zeros(env.env.nS) # initialize value-function\n",
        "  max_iterations = 10000\n",
        "  eps = 1e-20\n",
        "  for i in range(max_iterations):\n",
        "    prev_v = numpy.copy(value)\n",
        "    for s in range(env.env.nS): \n",
        "      q_sa = [sum([p * (r + prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in\n",
        "range(env.env.nA)]\n",
        "      value[s] = max(q_sa)\n",
        "    if (numpy.sum(numpy.fabs(prev_v - value)) <= eps):\n",
        "      print('Value-iteration converged at # %d.' % (i + 1))\n",
        "      break\n",
        "  return value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gamma = 1.0\n",
        "  env = gym.make(\"FrozenLake-v0\")\n",
        "  optimalValue = valueIteration(env, gamma);\n",
        "  startTime = time.time()\n",
        "  policy = calculatePolicy(optimalValue, gamma)\n",
        "  policy_score = evaluatePolicy(env, policy, gamma, n=1000)\n",
        "  endTime = time.time()\n",
        "  print(\"Best score = %0.2f. Time taken = %4.4f seconds\" % (numpy.mean(policy_score),\n",
        "endTime - startTime)) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value-iteration converged at # 1373.\n",
            "Best score = 0.73. Time taken = 0.3815 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LbchsQgAt3kH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code Practice \n",
        "The exercise of Dynamic Programming is from [this excercise](https://github.com/dennybritz/reinforcement-learning/tree/master/DP/)."
      ]
    },
    {
      "metadata": {
        "id": "Et1wJkfGtMg1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}