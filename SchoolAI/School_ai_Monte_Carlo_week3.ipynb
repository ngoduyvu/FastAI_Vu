{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "School_ai_Monte_Carlo_week3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "z0wQZD-qKBaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Review Theory\n",
        "**Markov Decision Processes**\n",
        "* State *S*\n",
        "* Action *A*\n",
        "* Transitions $P(s'|s,a)(or \\ T(s,a,s'))$\n",
        "* Rewards $R(s,a,s')(and \\ discount \\ \\gamma)$\n",
        "* Start State $s_o$\n",
        "\n",
        "Reinforcement Learning Type:\n",
        "* Known\n",
        "  * Current State\n",
        "  * Available Actions\n",
        "  * Experienced Rewards\n",
        " \n",
        "* Unknown\n",
        "  * Transition Model\n",
        "  * Reward Structure\n",
        "  \n",
        "* Assumed\n",
        "  * Markov Transitions\n",
        "  * Fixed Reward For (s,a,s')\n",
        "  \n",
        " Problem: Find Values for Fixed policy $\\pi$ (policy evaluation) <br>\n",
        " Model-based Learning: Learn the model, solve for values <br>\n",
        " Model-free Learning: Solve for values directly (by sampling) <br>\n",
        " \n",
        "** Monte Carlo Methods **\n",
        " Monte Carlo methods are a large family of computational algorithms that rely on *random sampling*. These methods are mainly used for:\n",
        "* Numerical integration\n",
        "* Stochastic optimization\n",
        "* Characterizing distributions\n",
        "\n",
        "Reason for using Monte Carlo vs Dynamic Programming\n",
        "* No need for a complete Markov Decision Process\n",
        "* Computationlly more efficient\n",
        "* Can be used with stochastic simulations\n",
        "\n",
        "** Monte Carlo Process: **\n",
        "* To evaluate state s\n",
        "* The first time-step t that state s is visited in an episode\n",
        "* Increment counter $N(s)  \\leftarrow N(s) + 1$\n",
        "* Increment total return $S(s) \\leftarrow S(s) + G_t$\n",
        "* Value is estimated by mean return $V(s)=\\frac{S(s)}{N(s)}$\n",
        "* By law of large numbers, $V(s) \\rightarrow v_\\pi(s) \\ as N(s) \\rightarrow \\infty$ \n",
        "\n",
        "** First-visit Monte Carlo policy evaluation ** <br>\n",
        "Initialize:\n",
        "> $pi \\leftarrow$ policy to be evaluated <br>\n",
        "> $V \\leftarrow$ an arbitrary state-value function\n",
        "> $Returns(s) \\leftarrow$ an empty list, for all $s \\in S$\n",
        "\n",
        "Repeat forever:\n",
        ">(a) Generate an episode using $\\pi$ <br>\n",
        ">(b) For each state s appearing in the episode:\n",
        ">>$R \\leftarrow$ following the first occurrence of s <br>\n",
        ">>Append R to *Returns(s)*\n",
        ">>$V(s) \\leftarrow$ average(Returns(s))\n",
        "\n",
        "In model-free reinforcement learning, as opposed to model based, we dont know the reward function and the transition function beforehand we have to learn them through experience.<br>\n",
        "In first visit monte carlo, the state value function is defined as the average of the returns following the agents first visit to s in a set of episodes."
      ]
    },
    {
      "metadata": {
        "id": "zgKBdgTaKBZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "5dVm5V8QOUS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}