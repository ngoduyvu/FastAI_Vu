{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "School_ai_Monte_Carlo_week3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "z0wQZD-qKBaz",
        "LW8goylO1vkp",
        "YbxziQ_3RBJj",
        "Gxjcw8QYh-X4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "z0wQZD-qKBaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Review Theory\n",
        "**Markov Decision Processes**\n",
        "* State *S*\n",
        "* Action *A*\n",
        "* Transitions $P(s'|s,a)(or \\ T(s,a,s'))$\n",
        "* Rewards $R(s,a,s')(and \\ discount \\ \\gamma)$\n",
        "* Start State $s_o$\n",
        "\n",
        "Reinforcement Learning Type:\n",
        "* Known\n",
        "  * Current State\n",
        "  * Available Actions\n",
        "  * Experienced Rewards\n",
        " \n",
        "* Unknown\n",
        "  * Transition Model\n",
        "  * Reward Structure\n",
        "  \n",
        "* Assumed\n",
        "  * Markov Transitions\n",
        "  * Fixed Reward For (s,a,s')\n",
        "  \n",
        " Problem: Find Values for Fixed policy $\\pi$ (policy evaluation) <br>\n",
        " Model-based Learning: Learn the model, solve for values <br>\n",
        " Model-free Learning: Solve for values directly (by sampling) <br>\n",
        " \n",
        "** Monte Carlo Methods **\n",
        " Monte Carlo methods are a large family of computational algorithms that rely on *random sampling*. These methods are mainly used for:\n",
        "* Numerical integration\n",
        "* Stochastic optimization\n",
        "* Characterizing distributions\n",
        "\n",
        "Reason for using Monte Carlo vs Dynamic Programming\n",
        "* No need for a complete Markov Decision Process\n",
        "* Computationlly more efficient\n",
        "* Can be used with stochastic simulations\n",
        "\n",
        "** Monte Carlo Process: **\n",
        "* To evaluate state s\n",
        "* The first time-step t that state s is visited in an episode\n",
        "* Increment counter $N(s)  \\leftarrow N(s) + 1$\n",
        "* Increment total return $S(s) \\leftarrow S(s) + G_t$\n",
        "* Value is estimated by mean return $V(s)=\\frac{S(s)}{N(s)}$\n",
        "* By law of large numbers, $V(s) \\rightarrow v_\\pi(s) \\ as N(s) \\rightarrow \\infty$ \n",
        "\n",
        "** First-visit Monte Carlo policy evaluation ** <br>\n",
        "Initialize:\n",
        "> $\\pi \\leftarrow$ policy to be evaluated <br>\n",
        "> $V \\leftarrow$ an arbitrary state-value function\n",
        "> $Returns(s) \\leftarrow$ an empty list, for all $s \\in S$\n",
        "\n",
        "Repeat forever:\n",
        ">(a) Generate an episode using $\\pi$ <br>\n",
        ">(b) For each state s appearing in the episode:\n",
        ">>$R \\leftarrow$ following the first occurrence of s <br>\n",
        ">>Append R to *Returns(s)*\n",
        ">>$V(s) \\leftarrow$ average(Returns(s))\n",
        "\n",
        "In model-free reinforcement learning, as opposed to model based, we dont know the reward function and the transition function beforehand we have to learn them through experience.<br>\n",
        "In first visit monte carlo, the state value function is defined as the average of the returns following the agents first visit to s in a set of episodes."
      ]
    },
    {
      "metadata": {
        "id": "LW8goylO1vkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploration vs Exploitation\n",
        "In Reinforcement learning an agent simultaneously attempts to acquire new knowledge (called \"exploration\") and optimizes its decision based on existing knowledge (called \"exploitation\"). The “exploration vs. exploitation tradeoff” applies systems that want to acquire new knowledge and maximize their reward at the same time. <br/>\n",
        "**Multi Arm Bandits (MAB):** \n",
        "Bandit problems embody in essential form a conflict evident in all human action: choosing actions which yield immediate reward vs. choosing actions (e.g. acquiring information or preparing the ground) whose benefit will come only later. <br>\n",
        "MAB is best understood through this analogy: A gambler at a row of slot machines has to decide which machines to play, how many times to play each machine and in which order to play them. When played, each machine provides a reward from a distribution specific to that machine. The objective is to maximize the sum of rewards earned through a sequence of lever pulls.\n",
        "\n",
        "  ![MAB](Img/MAB.png) <br>\n",
        "  Lets get formal and introduce some notation: <br>\n",
        "• Lets Index the arms by a, and the probability distribution over possible rewards r for each arm a can be written as $pa(r)$. <br>\n",
        "•  We have to find the arm with the largest mean reward  $μa=Ea[r]$. <br>\n",
        "• In practice pa(r) are non-stationary <br>\n",
        "\n",
        "So to come up with an optimal strategy to explore and exploit so as to reap maximum rewards, the model can follow 3 strategies: \n",
        "1. **Epsilon-Decreasing with Softmax** <br>\n",
        "With this strategy weexplore with probability epsilon, and exploit with probability 1 — epsilon. Epsilon decreases over time, in the case of exploring a new option, we don’t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that (this is the softmax part). In other words, we try to figure out what we want to do at a young age, and then stick with it throughout our lives. Throughout high school and college we explore a variety of subjects and are open to new experiences. The older we get the more likely we are to settle on a path, and major life or career changes become less likely.  In a sense, epsilon here models our risk aversion. As we become older we become more risk-averse and less likely to explore new options, like a major career change, even if they could yield high returns.\n",
        "2. **Upper-confidence bound strategy** <br>\n",
        "This stratey loosely corresponds to living a very optimistic life. In addition to estimating the outcome of each option, we also calculate the confidence of our estimation. This gives us an interval of possible outcomes. Now, here’s our strategy: We always pick the option with the highest possible outcome, even if it that outcome very unlikely. The intuition behind this strategy is that options with high uncertainty usually lead to a lot of new knowledge. We don’t know enough about the option to accurately estimate the return and by pursuing that option we are bound to learn more and improve our future estimations. In simulated settings this algorithm does well when we have many options with very different variances. \n",
        "3. **Contextual-Epsilon-greedy strategy** <br>\n",
        "This strategy is similar to epsilon-greedy, but we choose the value of epsilon based on how critical our situation is.When we are in a critical situation (large debt, need to provide for a sick family) we will always exploit instead of explore — We do what we know works well. If we are in a situation that is not critical we are more likely to explore new things. This strategy makes intuitive sense, but I believe that it is not commonly followed. Even in non-critcal situation we often choose to keep doing what we have always done due to our risk-averse nature."
      ]
    },
    {
      "metadata": {
        "id": "YbxziQ_3RBJj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo Reinforcement Learning Tutorial\n",
        "**Q Learning** <br>\n",
        "* $Q = Quality$ \n",
        "* $Q =$ Long-term discounted reward we expect from taking action a in state s \n",
        "* $Q(s,a)=R(s,a)+\\gamma V(s')$\n",
        "* $V(s)=max_a(R(s,a)+\\gamma V(s'))$\n",
        "* $\\pi(s)=max_a(Q(s,a))$\n",
        "\n",
        "**Policy**\n",
        "* Policy is a simple lookup table: state $\\rightarrow$ best action\n",
        "* Start with a random policy\n",
        "* Play the game, use experience to improve value estimates\n",
        "* Better value estimates improve policy\n",
        "\n",
        "**Returns (G)**\n",
        "* Return: the reward from our immediate action, plus all discounted future rewards from applying the current policy\n",
        "* Denoted by capital G\n",
        "* $G_t=r_{t+1}+\\gamma G_{t+1}$\n",
        "* Work in reverse from the final state applying this formula\n",
        "* Once (s, G) several pairs are collected, average them to estimate the value of each state\n",
        "\n",
        "**Algorithm to Calculate Returns** <br>\n",
        "1. Initialize G to 0\n",
        "2. states_and_returns =[]\n",
        "3. loop backwards through the list of states_and_rewards (s, r):\n",
        " 4. appends(s, G) to states_and_returns\n",
        " 5. $G = r+\\gamma*G$\n",
        "6. Reverse states_and_returns to the original order  \n",
        "\n",
        "**Explore / Exploit Dilemma**\n",
        "* We must strike a balance between explore / exploit\n",
        "* We are going to use a strategy called \"epsilon greedy\"\n",
        "* Epsilon is the probability that our agent will choose a random action instead of following policy\n",
        "\n",
        "**Epsilon Greedy Algorithm**\n",
        "1. generate a random number p, between 0 and 1\n",
        "2. if $p < (1-\\varepsilon)$ take the action dictated by policy\n",
        "3. otherwise take a random action\n",
        "\n",
        "**First Visit Optimization**\n",
        "* What happens if we visit the same state more than once?\n",
        "* It's been proven subsequent visits won't change the answer\n",
        "* All we need is the first visit\n",
        "* We throw the rest of the data away\n",
        "\n",
        "**Monte Carlo Q Learning Algorithm** <br>\n",
        "![Monte Carlo Algorithm](Img/Monte_Carlo_Algorithm.png)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Gxjcw8QYh-X4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implement Monte Carlo in Code"
      ]
    },
    {
      "metadata": {
        "id": "w_y1wNyFcClw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Grid World as an environment for reinforcement model\n",
        "# Code from https://github.com/colinskow/move37/blob/master/dynamic_programming/grid_world.py\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__(self, width, height, start):\n",
        "    # i is vertical axis, j is horizontal\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions, obey_prob):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "    self.obey_prob = obey_prob\n",
        "\n",
        "  def non_terminal_states(self):\n",
        "    return self.actions.keys()\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  def stochastic_move(self, action):\n",
        "    p = np.random.random()\n",
        "    if p <= self.obey_prob:\n",
        "      return action\n",
        "    if action == 'U' or action == 'D':\n",
        "      return np.random.choice(['L', 'R'])\n",
        "    elif action == 'L' or action == 'R':\n",
        "      return np.random.choice(['U', 'D'])\n",
        "\n",
        "  def move(self, action):\n",
        "    actual_action = self.stochastic_move(action)\n",
        "    if actual_action in self.actions[(self.i, self.j)]:\n",
        "      if actual_action == 'U':\n",
        "        self.i -= 1\n",
        "      elif actual_action == 'D':\n",
        "        self.i += 1\n",
        "      elif actual_action == 'R':\n",
        "        self.j += 1\n",
        "      elif actual_action == 'L':\n",
        "        self.j -= 1\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def check_move(self, action):\n",
        "    i = self.i\n",
        "    j = self.j\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        i -= 1\n",
        "      elif action == 'D':\n",
        "        i += 1\n",
        "      elif action == 'R':\n",
        "        j += 1\n",
        "      elif action == 'L':\n",
        "        j -= 1\n",
        "    # return a reward (if any)\n",
        "    reward = self.rewards.get((i, j), 0)\n",
        "    return ((i, j), reward)\n",
        "\n",
        "  def get_transition_probs(self, action):\n",
        "    # returns a list of (probability, reward, s') transition tuples\n",
        "    probs = []\n",
        "    state, reward = self.check_move(action)\n",
        "    probs.append((self.obey_prob, reward, state))\n",
        "    disobey_prob = 1 - self.obey_prob\n",
        "    if not (disobey_prob > 0.0):\n",
        "      return probs\n",
        "    if action == 'U' or action == 'D':\n",
        "      state, reward = self.check_move('L')\n",
        "      probs.append((disobey_prob / 2, reward, state))\n",
        "      state, reward = self.check_move('R')\n",
        "      probs.append((disobey_prob / 2, reward, state))\n",
        "    elif action == 'L' or action == 'R':\n",
        "      state, reward = self.check_move('U')\n",
        "      probs.append((disobey_prob / 2, reward, state))\n",
        "      state, reward = self.check_move('D')\n",
        "      probs.append((disobey_prob / 2, reward, state))\n",
        "    return probs\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid(obey_prob=1.0, step_cost=None):\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  # the grid looks like this\n",
        "  # x means you can't go there\n",
        "  # s means start position\n",
        "  # number means reward at that state\n",
        "  # .  .  .  1\n",
        "  # .  x  . -1\n",
        "  # s  .  .  .\n",
        "  # obey_brob (float): the probability of obeying the command\n",
        "  # step_cost (float): a penalty applied each step to minimize the number of moves (-0.1)\n",
        "  g = Grid(3, 4, (2, 0))\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),\n",
        "    (0, 1): ('L', 'R'),\n",
        "    (0, 2): ('L', 'D', 'R'),\n",
        "    (1, 0): ('U', 'D'),\n",
        "    (1, 2): ('U', 'D', 'R'),\n",
        "    (2, 0): ('U', 'R'),\n",
        "    (2, 1): ('L', 'R'),\n",
        "    (2, 2): ('L', 'R', 'U'),\n",
        "    (2, 3): ('L', 'U'),\n",
        "  }\n",
        "  g.set(rewards, actions, obey_prob)\n",
        "  if step_cost is not None:\n",
        "    g.rewards.update({\n",
        "      (0, 0): step_cost,\n",
        "      (0, 1): step_cost,\n",
        "      (0, 2): step_cost,\n",
        "      (1, 0): step_cost,\n",
        "      (1, 2): step_cost,\n",
        "      (2, 0): step_cost,\n",
        "      (2, 1): step_cost,\n",
        "      (2, 2): step_cost,\n",
        "      (2, 3): step_cost,\n",
        "    })\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h87bctkjlX3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code from https://github.com/colinskow/move37/blob/master/dynamic_programming/utils.py\n",
        "# Forked from https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl\n",
        "# Credit goes to LazyProgrammer\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YXWDYV7it2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "5f024fd4-274f-423b-fcc4-1617f7485406"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from grid_world import standard_grid\n",
        "#from utils import max_dict, print_values, print_policy\n",
        "\n",
        "GAMMA = 0.9\n",
        "EPSILON=0.2\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "N_EPISODES = 10000\n",
        "\n",
        "# epsilon greedy action selection\n",
        "def epsilon_action(a, eps=0.1):\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  a = epsilon_action(policy[s], EPSILON)\n",
        "\n",
        "  # keep in mind that reward is lagged by one time step\n",
        "  # r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = epsilon_action(policy[s], EPSILON)\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # a terminal state has a value of 0 by definition\n",
        "    # this is the first state we encounter in the reversed list\n",
        "    # we'll ignore its return (G) since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # back to the original order of states visited\n",
        "  return states_actions_returns\n",
        "\n",
        "\n",
        "def monte_carlo(grid):\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.non_terminal_states()\n",
        "  for s in states:\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0\n",
        "      returns[(s,a)] = []\n",
        "  \n",
        "  # keep track of how much our Q values change each episode so we can know when it converges\n",
        "  deltas = []\n",
        "  # repeat for the number of episodes specified (enough that it converges)\n",
        "  for t in range(N_EPISODES):\n",
        "    if t % 1000 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using the current policy\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "\n",
        "    # calculate Q(s,a)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # first-visit Monte Carlo optimization\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        returns[sa].append(G)\n",
        "        old_q = Q[s][a]\n",
        "        # the new Q[s][a] is the sample mean of all our returns for that (state, action)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "    for s in policy.keys():\n",
        "      a, _ = max_dict(Q[s])\n",
        "      policy[s] = a\n",
        "  \n",
        "  # calculate values for each state (just to print and compare)\n",
        "  # V(s) = max[a]{ Q(s,a) }\n",
        "  V = {}\n",
        "  for s in policy.keys():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "  \n",
        "  return V, policy, deltas\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  grid = standard_grid(obey_prob=1.0, step_cost=None)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  V, policy, deltas = monte_carlo(grid)\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "final values:\n",
            "---------------------------\n",
            " 0.76| 0.87| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.67| 0.00| 0.87| 0.00|\n",
            "---------------------------\n",
            " 0.59| 0.60| 0.70| 0.68|\n",
            "final policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd5edfd2cc0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gmTk4xNM-Qb0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q Learning\n",
        "A partially observable markov decicion process is one where we don't know what the true state looks like, but we can observe a part of it.<br>\n",
        "A _Q table_ is one where the states are rows and actions are columns, it helps us find the best action to take for each state.<br>\n",
        "_Q Learning_, is the process of learning what this Q table is directly, without needing to learn either the transition probability or reward function."
      ]
    },
    {
      "metadata": {
        "id": "85acs7Y-a3vT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run TPU on Colab"
      ]
    },
    {
      "metadata": {
        "id": "I1UjAHsDWm-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8e4248ba-a6ce-4f11-e1c2-cc5d9855f4a5"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# TPU Benchmarch\n",
        "tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU Address is', tpu_address)\n",
        "\n",
        "with tf.Session(tpu_address) as session:\n",
        "  devices = session.list_devices()\n",
        "  \n",
        "print('TPU Devices:', devices)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU Address is grpc://10.99.210.154:8470\n",
            "TPU Devices: [_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 1324157668015504541), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17925830785529702466), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 6943238829902747458), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 897833771215715781), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6925443887496325425), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15941686158536005639), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3666343166592646238), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12104073280341908582), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5434887718980406359), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3469652314594526213), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10081731406949266922), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3488726400282007778)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AxW0fJc1XUkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8e925b90-e91a-4a0e-dc54-3d7b1d6b8a8a"
      },
      "cell_type": "code",
      "source": [
        "# TPU Run\n",
        "def add_op(x, y):\n",
        "  return x + y\n",
        "\n",
        "x = tf.placeholder(tf.float32, [100,])\n",
        "y = tf.placeholder(tf.float32, [100,])\n",
        "tpu_ops = tf.contrib.tpu.rewrite(add_op, [x,y])\n",
        "\n",
        "session = tf.Session(tpu_address)\n",
        "\n",
        "try:\n",
        "  session.run(tf.contrib.tpu.initialize_system())\n",
        "  start = time.time()\n",
        "  print(session.run(tpu_ops, {x: np.arange(100), y: np.arange(100)}))\n",
        "  end = time.time()\n",
        "  elapsed = end -start\n",
        "  print(elapsed)\n",
        "finally:\n",
        "  session.run(tf.contrib.tpu.shutdown_system(),)\n",
        "  session.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,\n",
            "        22.,  24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,\n",
            "        44.,  46.,  48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,\n",
            "        66.,  68.,  70.,  72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,\n",
            "        88.,  90.,  92.,  94.,  96.,  98., 100., 102., 104., 106., 108.,\n",
            "       110., 112., 114., 116., 118., 120., 122., 124., 126., 128., 130.,\n",
            "       132., 134., 136., 138., 140., 142., 144., 146., 148., 150., 152.,\n",
            "       154., 156., 158., 160., 162., 164., 166., 168., 170., 172., 174.,\n",
            "       176., 178., 180., 182., 184., 186., 188., 190., 192., 194., 196.,\n",
            "       198.], dtype=float32)]\n",
            "0.01967453956604004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yUURrQCIaQBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c5c24a69-90e0-4271-fd50-762b799bc6d2"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "def basic_mlp_module(input, units):\n",
        "    x = Dense(units)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n",
        "def create_mlp_model():\n",
        "    input = Input((32*32*3,))\n",
        "    x = basic_mlp_module(input, 2048)\n",
        "    x = basic_mlp_module(x, 1024)\n",
        "    x = basic_mlp_module(x, 512)\n",
        "    x = basic_mlp_module(x, 256)\n",
        "    x = basic_mlp_module(x, 128)\n",
        "    x = basic_mlp_module(x, 64)\n",
        "    x = basic_mlp_module(x, 32)\n",
        "    x = basic_mlp_module(x, 16)\n",
        "    x = Dense(10, activation=\"softmax\")(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def main():\n",
        "    # 7 lets make this into TPU Code!!!\n",
        "    K.clear_session()\n",
        "\n",
        "    # CIFAR\n",
        "    (X_train, y_train), (_, _) = cifar10.load_data()\n",
        "    X_train = (X_train / 255.0).reshape(50000, -1)\n",
        "    y_train = to_categorical(y_train)\n",
        "\n",
        "    # Model building\n",
        "    model = create_mlp_model()\n",
        "    model.compile(tf.train.AdamOptimizer(learning_rate=1e-3), loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=1024, epochs=10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 55s 0us/step\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 48s 964us/step - loss: 2.5892 - acc: 0.1004\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 45s 906us/step - loss: 2.3921 - acc: 0.1030\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 46s 911us/step - loss: 2.3278 - acc: 0.1117\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 46s 921us/step - loss: 2.2989 - acc: 0.1206\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 47s 939us/step - loss: 2.2623 - acc: 0.1368\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 46s 925us/step - loss: 2.2223 - acc: 0.1530\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 46s 924us/step - loss: 2.1844 - acc: 0.1627\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 46s 926us/step - loss: 2.1455 - acc: 0.1715\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 46s 917us/step - loss: 2.1099 - acc: 0.1741\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 46s 911us/step - loss: 2.0810 - acc: 0.1787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OYujE1yxbUS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}