{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "School_ai_Monte_Carlo_week3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "z0wQZD-qKBaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Review Theory\n",
        "**Markov Decision Processes**\n",
        "* State *S*\n",
        "* Action *A*\n",
        "* Transitions $P(s'|s,a)(or \\ T(s,a,s'))$\n",
        "* Rewards $R(s,a,s')(and \\ discount \\ \\gamma)$\n",
        "* Start State $s_o$\n",
        "\n",
        "Reinforcement Learning Type:\n",
        "* Known\n",
        "  * Current State\n",
        "  * Available Actions\n",
        "  * Experienced Rewards\n",
        " \n",
        "* Unknown\n",
        "  * Transition Model\n",
        "  * Reward Structure\n",
        "  \n",
        "* Assumed\n",
        "  * Markov Transitions\n",
        "  * Fixed Reward For (s,a,s')\n",
        "  \n",
        " Problem: Find Values for Fixed policy $\\pi$ (policy evaluation) <br>\n",
        " Model-based Learning: Learn the model, solve for values <br>\n",
        " Model-free Learning: Solve for values directly (by sampling) <br>\n",
        " \n",
        "** Monte Carlo Methods **\n",
        " Monte Carlo methods are a large family of computational algorithms that rely on *random sampling*. These methods are mainly used for:\n",
        "* Numerical integration\n",
        "* Stochastic optimization\n",
        "* Characterizing distributions\n",
        "\n",
        "Reason for using Monte Carlo vs Dynamic Programming\n",
        "* No need for a complete Markov Decision Process\n",
        "* Computationlly more efficient\n",
        "* Can be used with stochastic simulations\n",
        "\n",
        "** Monte Carlo Process: **\n",
        "* To evaluate state s\n",
        "* The first time-step t that state s is visited in an episode\n",
        "* Increment counter $N(s)  \\leftarrow N(s) + 1$\n",
        "* Increment total return $S(s) \\leftarrow S(s) + G_t$\n",
        "* Value is estimated by mean return $V(s)=\\frac{S(s)}{N(s)}$\n",
        "* By law of large numbers, $V(s) \\rightarrow v_\\pi(s) \\ as N(s) \\rightarrow \\infty$ \n",
        "\n",
        "** First-visit Monte Carlo policy evaluation ** <br>\n",
        "Initialize:\n",
        "> $pi \\leftarrow$ policy to be evaluated <br>\n",
        "> $V \\leftarrow$ an arbitrary state-value function\n",
        "> $Returns(s) \\leftarrow$ an empty list, for all $s \\in S$\n",
        "\n",
        "Repeat forever:\n",
        ">(a) Generate an episode using $\\pi$ <br>\n",
        ">(b) For each state s appearing in the episode:\n",
        ">>$R \\leftarrow$ following the first occurrence of s <br>\n",
        ">>Append R to *Returns(s)*\n",
        ">>$V(s) \\leftarrow$ average(Returns(s))\n",
        "\n",
        "In model-free reinforcement learning, as opposed to model based, we dont know the reward function and the transition function beforehand we have to learn them through experience.<br>\n",
        "In first visit monte carlo, the state value function is defined as the average of the returns following the agents first visit to s in a set of episodes."
      ]
    },
    {
      "metadata": {
        "id": "LW8goylO1vkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploration vs Exploitation\n",
        "In Reinforcement learning an agent simultaneously attempts to acquire new knowledge (called \"exploration\") and optimizes its decision based on existing knowledge (called \"exploitation\"). The “exploration vs. exploitation tradeoff” applies systems that want to acquire new knowledge and maximize their reward at the same time. <br/>\n",
        "**Multi Arm Bandits (MAB):** \n",
        "Bandit problems embody in essential form a conflict evident in all human action: choosing actions which yield immediate reward vs. choosing actions (e.g. acquiring information or preparing the ground) whose benefit will come only later. <br>\n",
        "MAB is best understood through this analogy: A gambler at a row of slot machines has to decide which machines to play, how many times to play each machine and in which order to play them. When played, each machine provides a reward from a distribution specific to that machine. The objective is to maximize the sum of rewards earned through a sequence of lever pulls.\n",
        "\n",
        "  ![MAB](Img/MAB.png) <br>\n",
        "  Lets get formal and introduce some notation: <br>\n",
        "• Lets Index the arms by a, and the probability distribution over possible rewards r for each arm a can be written as $pa(r)$. <br>\n",
        "•  We have to find the arm with the largest mean reward  $μa=Ea[r]$. <br>\n",
        "• In practice pa(r) are non-stationary <br>\n",
        "\n",
        "So to come up with an optimal strategy to explore and exploit so as to reap maximum rewards, the model can follow 3 strategies: \n",
        "1. **Epsilon-Decreasing with Softmax** <br>\n",
        "With this strategy weexplore with probability epsilon, and exploit with probability 1 — epsilon. Epsilon decreases over time, in the case of exploring a new option, we don’t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that (this is the softmax part). In other words, we try to figure out what we want to do at a young age, and then stick with it throughout our lives. Throughout high school and college we explore a variety of subjects and are open to new experiences. The older we get the more likely we are to settle on a path, and major life or career changes become less likely.  In a sense, epsilon here models our risk aversion. As we become older we become more risk-averse and less likely to explore new options, like a major career change, even if they could yield high returns.\n",
        "2. **Upper-confidence bound strategy** <br>\n",
        "This stratey loosely corresponds to living a very optimistic life. In addition to estimating the outcome of each option, we also calculate the confidence of our estimation. This gives us an interval of possible outcomes. Now, here’s our strategy: We always pick the option with the highest possible outcome, even if it that outcome very unlikely. The intuition behind this strategy is that options with high uncertainty usually lead to a lot of new knowledge. We don’t know enough about the option to accurately estimate the return and by pursuing that option we are bound to learn more and improve our future estimations. In simulated settings this algorithm does well when we have many options with very different variances. \n",
        "3. **Contextual-Epsilon-greedy strategy** <br>\n",
        "This strategy is similar to epsilon-greedy, but we choose the value of epsilon based on how critical our situation is.When we are in a critical situation (large debt, need to provide for a sick family) we will always exploit instead of explore — We do what we know works well. If we are in a situation that is not critical we are more likely to explore new things. This strategy makes intuitive sense, but I believe that it is not commonly followed. Even in non-critcal situation we often choose to keep doing what we have always done due to our risk-averse nature."
      ]
    },
    {
      "metadata": {
        "id": "YbxziQ_3RBJj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo Reinforcement Learning Tutorial\n",
        "**Q Learning** <br>\n",
        "* $Q = Quality$ \n",
        "* $Q =$ Long-term discounted reward we expect from taking action a in state s \n",
        "* $Q(s,a)=R(s,a)+\\gamma V(s')$\n",
        "* $V(s)=max_a(R(s,a)+\\gamma V(s'))$\n",
        "* $\\pi(s)=max_a(Q(s,a))$\n",
        "\n",
        "**Policy**\n",
        "* Policy is a simple lookup table: state $\\rightarrow$ best action\n",
        "* Start with a random policy\n",
        "* Play the game, use experience to improve value estimates\n",
        "* Better value estimates improve policy\n",
        "\n",
        "**Returns (G)**\n",
        "* Return: the reward from our immediate action, plus all discounted future rewards from applying the current policy\n",
        "* Denoted by capital G\n",
        "* $G_t=r_{t+1}+\\gamma G_{t+1}$\n",
        "* Work in reverse from the final state applying this formula\n",
        "* Once (s, G) several pairs are collected, average them to estimate the value of each state\n",
        "\n",
        "**Algorithm to Calculate Returns** <br>\n",
        "1. Initialize G to 0\n",
        "2. states_and_returns =[]\n",
        "3. loop backwards through the list of states_and_rewards (s, r):\n",
        " 4. appends(s, G) to states_and_returns\n",
        " 5. $G = r+\\gamma*G$\n",
        "6. Reverse states_and_returns to the original order  \n",
        "\n",
        "**Explore / Exploit Dilemma**\n",
        "* We must strike a balance between explore / exploit\n",
        "* We are going to use a strategy called \"epsilon greedy\"\n",
        "* Epsilon is the probability that our agent will choose a random action instead of following policy\n",
        "\n",
        "**Epsilon Greedy Algorithm**\n",
        "1. generate a random number p, between 0 and 1\n",
        "2. if $p < (1-\\varepsilon)$ take the action dictated by policy\n",
        "3. otherwise take a random action\n",
        "\n",
        "**First Visit Optimization**\n",
        "* What happens if we visit the same state more than once?\n",
        "* It's been proven subsequent visits won't change the answer\n",
        "* All we need is the first visit\n",
        "* We throw the rest of the data away\n",
        "\n",
        "**Monte Carlo Q Learning Algorithm** <br>\n",
        "![Monte Carlo Algorithm](Img/Monte_Carlo_Algorithm.png)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "w_y1wNyFcClw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}